{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e51756ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, recall_score, classification_report\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.pipeline import Pipeline, make_pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b6ac4f95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>G</th>\n",
       "      <th>AB</th>\n",
       "      <th>PA</th>\n",
       "      <th>H</th>\n",
       "      <th>1B</th>\n",
       "      <th>2B</th>\n",
       "      <th>3B</th>\n",
       "      <th>HR</th>\n",
       "      <th>R</th>\n",
       "      <th>...</th>\n",
       "      <th>Events</th>\n",
       "      <th>CStr%</th>\n",
       "      <th>CSW%</th>\n",
       "      <th>xBA</th>\n",
       "      <th>xSLG</th>\n",
       "      <th>xwOBA</th>\n",
       "      <th>L-WAR</th>\n",
       "      <th>avg_wOBA</th>\n",
       "      <th>zscore_difference_woba</th>\n",
       "      <th>z_scores_avg_wOBA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37.666667</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>454.0</td>\n",
       "      <td>502.666667</td>\n",
       "      <td>120.333333</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>15.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.666667</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>351.333333</td>\n",
       "      <td>0.152667</td>\n",
       "      <td>0.268000</td>\n",
       "      <td>0.246000</td>\n",
       "      <td>0.395667</td>\n",
       "      <td>0.307000</td>\n",
       "      <td>-0.766667</td>\n",
       "      <td>0.299333</td>\n",
       "      <td>-0.163228</td>\n",
       "      <td>-0.841291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32.666667</td>\n",
       "      <td>127.666667</td>\n",
       "      <td>436.0</td>\n",
       "      <td>483.666667</td>\n",
       "      <td>113.666667</td>\n",
       "      <td>68.666667</td>\n",
       "      <td>29.666667</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>10.666667</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>341.333333</td>\n",
       "      <td>0.157000</td>\n",
       "      <td>0.266333</td>\n",
       "      <td>0.246000</td>\n",
       "      <td>0.374333</td>\n",
       "      <td>0.303333</td>\n",
       "      <td>1.266667</td>\n",
       "      <td>0.322333</td>\n",
       "      <td>-0.575723</td>\n",
       "      <td>-0.106384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34.666667</td>\n",
       "      <td>149.000000</td>\n",
       "      <td>523.0</td>\n",
       "      <td>617.000000</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>77.666667</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>426.666667</td>\n",
       "      <td>0.166333</td>\n",
       "      <td>0.249000</td>\n",
       "      <td>0.254333</td>\n",
       "      <td>0.446333</td>\n",
       "      <td>0.351333</td>\n",
       "      <td>1.766667</td>\n",
       "      <td>0.327333</td>\n",
       "      <td>-0.459862</td>\n",
       "      <td>0.053379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32.666667</td>\n",
       "      <td>148.000000</td>\n",
       "      <td>532.0</td>\n",
       "      <td>612.333333</td>\n",
       "      <td>131.666667</td>\n",
       "      <td>85.666667</td>\n",
       "      <td>26.666667</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>17.666667</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>396.000000</td>\n",
       "      <td>0.204333</td>\n",
       "      <td>0.289000</td>\n",
       "      <td>0.253667</td>\n",
       "      <td>0.425333</td>\n",
       "      <td>0.338333</td>\n",
       "      <td>1.733333</td>\n",
       "      <td>0.323667</td>\n",
       "      <td>0.649372</td>\n",
       "      <td>-0.063780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30.666667</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>491.0</td>\n",
       "      <td>579.000000</td>\n",
       "      <td>125.666667</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>24.333333</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>79.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>403.000000</td>\n",
       "      <td>0.168667</td>\n",
       "      <td>0.257000</td>\n",
       "      <td>0.265667</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.361333</td>\n",
       "      <td>2.733333</td>\n",
       "      <td>0.360333</td>\n",
       "      <td>-1.456233</td>\n",
       "      <td>1.107812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 316 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Age           G     AB          PA           H         1B         2B  \\\n",
       "0  37.666667  126.000000  454.0  502.666667  120.333333  94.000000  15.666667   \n",
       "1  32.666667  127.666667  436.0  483.666667  113.666667  68.666667  29.666667   \n",
       "2  34.666667  149.000000  523.0  617.000000  123.000000  77.666667  21.000000   \n",
       "3  32.666667  148.000000  532.0  612.333333  131.666667  85.666667  26.666667   \n",
       "4  30.666667  139.000000  491.0  579.000000  125.666667  72.000000  24.333333   \n",
       "\n",
       "         3B         HR          R  ...      Events     CStr%      CSW%  \\\n",
       "0  0.000000  10.666667  38.000000  ...  351.333333  0.152667  0.268000   \n",
       "1  4.666667  10.666667  48.000000  ...  341.333333  0.157000  0.266333   \n",
       "2  0.333333  24.000000  76.000000  ...  426.666667  0.166333  0.249000   \n",
       "3  1.666667  17.666667  80.000000  ...  396.000000  0.204333  0.289000   \n",
       "4  2.333333  27.000000  79.666667  ...  403.000000  0.168667  0.257000   \n",
       "\n",
       "        xBA      xSLG     xwOBA     L-WAR  avg_wOBA  zscore_difference_woba  \\\n",
       "0  0.246000  0.395667  0.307000 -0.766667  0.299333               -0.163228   \n",
       "1  0.246000  0.374333  0.303333  1.266667  0.322333               -0.575723   \n",
       "2  0.254333  0.446333  0.351333  1.766667  0.327333               -0.459862   \n",
       "3  0.253667  0.425333  0.338333  1.733333  0.323667                0.649372   \n",
       "4  0.265667  0.458333  0.361333  2.733333  0.360333               -1.456233   \n",
       "\n",
       "   z_scores_avg_wOBA  \n",
       "0          -0.841291  \n",
       "1          -0.106384  \n",
       "2           0.053379  \n",
       "3          -0.063780  \n",
       "4           1.107812  \n",
       "\n",
       "[5 rows x 316 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df = pd.read_csv('Resources/dash_full_batter_data.csv')\n",
    "batter_woba_df = pd.read_csv(\"Resources/full_woba_learning_name.csv\")\n",
    "batter_woba = batter_woba_df.drop(['Name', 'z_scores_avg_woba', 'z_scores_avg_slg', 'z_scores_avg_babip', 'z_scores_avg_wrc+', 'z_scores_wOBA_2023', 'zscore_difference_wOBA_2023' ], axis=1)\n",
    "batter_woba.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cd097195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_logistic_regression(df, target_column, solver='lbfgs', max_iter=100):\n",
    "    # Extract names\n",
    "    \n",
    "    # Drop rows with missing values\n",
    "    df.dropna(axis=1, inplace=True)\n",
    "    \n",
    "    # Convert the target column into binary classes (0 or 1)\n",
    "    df[target_column] = df[target_column].apply(lambda x: 1 if x > 0 else 0)\n",
    "    \n",
    "    # Split data into features (X) and target (y)\n",
    "    X = df.drop([target_column], axis=1)  # Exclude 'Name'\n",
    "    y = df[target_column]\n",
    "    \n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    \n",
    "    # Apply SMOTE for oversampling\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Standardize features using StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Create the Logistic Regression model\n",
    "    logreg_model = LogisticRegression(solver=solver, max_iter=max_iter)\n",
    "    \n",
    "    # Train the model\n",
    "    logreg_model.fit(X_train_scaled, y_train_resampled)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = logreg_model.predict(X_test_scaled)\n",
    "    \n",
    "    # Create the SHAP explainer with the trained model\n",
    "    explainer = shap.Explainer(logreg_model, X_train_scaled)\n",
    "    \n",
    "    # Calculate SHAP values\n",
    "    shap_values = explainer(X_test_scaled)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"balanced_recall\": balanced_accuracy_score(y_test, y_pred),\n",
    "        \"shap_values\": shap_values,\n",
    "        \"X_test_scaled\": X_test_scaled,\n",
    "        \"y_test\": y_test,\n",
    "        \"y_pred\": y_pred,\n",
    "        \"X_train\": X_train,\n",
    "        \"X_train_scaled\": X_train_scaled,\n",
    "        \"y_train_resampled\": y_train_resampled,\n",
    "        \"X_test\": X_test  # Add this line to include X_test in the result\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "905996fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.62\n",
      "Balanced Recall: 0.62\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a DataFrame named batter_woba_df\n",
    "target_column = 'zscore_difference_woba'\n",
    "result_woba = train_and_evaluate_logistic_regression(batter_woba, target_column, solver='lbfgs', max_iter=200)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Accuracy: {result_woba['accuracy']:.2f}\")\n",
    "print(f\"Balanced Recall: {result_woba['balanced_recall']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e3ec1f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the predictions and actual labels for both training and testing sets\n",
    "# y_pred_train = result_woba['y_train_resampled']\n",
    "# y_pred_test = result_woba['y_pred']\n",
    "# y_train = result_woba['y_train_resampled']\n",
    "# y_test = result_woba['y_test']\n",
    "\n",
    "# # Get the indexes of the DataFrames X_train and X_test\n",
    "# indexes_train = result_woba['X_train'].index\n",
    "# indexes_test = result_woba['X_test'].index\n",
    "\n",
    "# # Attach the 'Name' feature to the predictions for training set\n",
    "# names_train = full_df.loc[indexes_train, 'Name']\n",
    "# predictions_with_names_train = list(zip(names_train, y_pred_train, y_train))\n",
    "\n",
    "# # Attach the 'Name' feature to the predictions for testing set\n",
    "# names_test = full_df.loc[indexes_test, 'Name']\n",
    "# predictions_with_names_test = list(zip(names_test, y_pred_test, y_test))\n",
    "\n",
    "# # Create DataFrames for the predictions with names for training and testing sets\n",
    "# predictions_df_train = pd.DataFrame(predictions_with_names_train, columns=['Name', 'Prediction', 'Actual'])\n",
    "# predictions_df_test = pd.DataFrame(predictions_with_names_test, columns=['Name', 'Prediction', 'Actual'])\n",
    "\n",
    "# # Concatenate the DataFrames vertically\n",
    "# pred_actual_df = pd.concat([predictions_df_train, predictions_df_test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3866d8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_predictions_dataframe(result_df, name_df):\n",
    "    # Get the predictions and actual labels for both training and testing sets\n",
    "    y_pred_train = result_df['y_train_resampled']\n",
    "    y_pred_test = result_df['y_pred']\n",
    "    y_train = result_df['y_train_resampled']\n",
    "    y_test = result_df['y_test']\n",
    "\n",
    "    # Get the indexes of the DataFrames X_train and X_test\n",
    "    indexes_train = result_df['X_train'].index\n",
    "    indexes_test = result_df['X_test'].index\n",
    "\n",
    "    # Attach the 'Name' feature to the predictions for training set\n",
    "    names_train = name_df.loc[indexes_train, 'Name']\n",
    "    predictions_with_names_train = list(zip(names_train, y_pred_train, y_train))\n",
    "\n",
    "    # Attach the 'Name' feature to the predictions for testing set\n",
    "    names_test = name_df.loc[indexes_test, 'Name']\n",
    "    predictions_with_names_test = list(zip(names_test, y_pred_test, y_test))\n",
    "\n",
    "    # Create DataFrames for the predictions with names for training and testing sets\n",
    "    predictions_df_train = pd.DataFrame(predictions_with_names_train, columns=['Name', 'Prediction', 'Actual'])\n",
    "    predictions_df_test = pd.DataFrame(predictions_with_names_test, columns=['Name', 'Prediction', 'Actual'])\n",
    "\n",
    "    # Concatenate the DataFrames vertically\n",
    "    pred_actual_df = pd.concat([predictions_df_train, predictions_df_test], ignore_index=True)\n",
    "    \n",
    "    return pred_actual_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f3e1a7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_actual_woba = create_predictions_dataframe(result_woba, full_df)\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "pred_actual_woba.to_csv('predictions_woba.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
